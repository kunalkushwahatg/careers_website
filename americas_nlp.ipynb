{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f25219f-7236-4997-94a9-cf290e91da60",
   "metadata": {
    "id": "3f25219f-7236-4997-94a9-cf290e91da60"
   },
   "outputs": [],
   "source": [
    "lang1 = \"/content/traincni.txt\"\n",
    "lang2 = \"/content/traines.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "046e61da-8681-4d3b-9ffb-b050a6533308",
   "metadata": {
    "id": "046e61da-8681-4d3b-9ffb-b050a6533308"
   },
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import re\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b72be92c-d555-454f-8273-99d9d4fc9c53",
   "metadata": {
    "id": "b72be92c-d555-454f-8273-99d9d4fc9c53"
   },
   "outputs": [],
   "source": [
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "BATCH_SIZE = 32\n",
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
    "        self.n_words = 2  # Count SOS and EOS\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f765ffd5-3f0b-4b6e-b648-9a86b97aba17",
   "metadata": {
    "id": "f765ffd5-3f0b-4b6e-b648-9a86b97aba17"
   },
   "outputs": [],
   "source": [
    "# Turn a Unicode string to plain ASCII, thanks to\n",
    "# https://stackoverflow.com/a/518232/2809427\n",
    "\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "# Lowercase, trim, and remove non-letter characters\n",
    "def normalizeString(s):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z!?]+\", r\" \", s)\n",
    "    return s.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "02374cc8-4a38-416d-a685-550de494536c",
   "metadata": {
    "id": "02374cc8-4a38-416d-a685-550de494536c"
   },
   "outputs": [],
   "source": [
    "\n",
    "    # Read the file and split into lines\n",
    "l1 = open(lang1, encoding='utf-8').\\\n",
    "        read().split('\\n')\n",
    "l2 = open(lang2, encoding='utf-8').\\\n",
    "        read().split('\\n')\n",
    "\n",
    "#     # Split every line into pairs and normalize\n",
    "# pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\n",
    "\n",
    "#     # Reverse pairs, make Lang instances\n",
    "# if reverse:\n",
    "#     pairs = [list(reversed(p)) for p in pairs]\n",
    "#     input_lang = Lang(lang2)\n",
    "#     output_lang = Lang(lang1)\n",
    "# else:\n",
    "#     input_lang = Lang(lang1)\n",
    "#     output_lang = Lang(lang2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ad54c032-2523-42d0-80ee-57c6eaef6776",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "ad54c032-2523-42d0-80ee-57c6eaef6776",
    "outputId": "8fd91608-f925-4cab-c11d-7e133660b6e9"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'SANKENARENTSI ASHI TARAPOTO'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "832eabef-a704-4399-a851-ea3f0299d1ca",
   "metadata": {
    "id": "832eabef-a704-4399-a851-ea3f0299d1ca"
   },
   "outputs": [],
   "source": [
    "pairs = []\n",
    "for i in range(len(l1)):\n",
    "    pairs.append([l1[i],l2[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "99d382d8-ea2f-4445-8b5f-fa8e5da21c17",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "99d382d8-ea2f-4445-8b5f-fa8e5da21c17",
    "outputId": "9d793fda-b3e1-4f4f-f5d0-d5902c5aa839"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SANKENARENTSI ASHI TARAPOTO', 'CARTA DE TARAPOTO']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7635069e-a3f8-4d55-9fd9-8933c3d8498a",
   "metadata": {
    "id": "7635069e-a3f8-4d55-9fd9-8933c3d8498a"
   },
   "outputs": [],
   "source": [
    "MAX_LENGTH = 30\n",
    "def filterPairs(p):\n",
    "    return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
    "        len(p[1].split(' ')) < MAX_LENGTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "190a76db-e5f1-437d-9b52-bf1c1fe2b390",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "190a76db-e5f1-437d-9b52-bf1c1fe2b390",
    "outputId": "f070a579-689a-44df-95d4-7fa65d4a8384"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 0 sentence pairs\n",
      "Trimmed to 3674 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "es 11706\n",
      "cni 8952\n",
      "['Nomokakiro, nosantzimaitaro tyoshiki, noshintsatziro tyoshikipaye, chovankiriki, taanoki, poñaanaka iroka tsoiroki.', 'Hago un hueco, los ensarto pini-pini, huaruro, zapatitos, después, este caracol.']\n"
     ]
    }
   ],
   "source": [
    "def prepareData(pairsn):\n",
    "    pairs = []\n",
    "    input_lang = Lang('es')\n",
    "    output_lang = Lang('cni')\n",
    "    print(\"Read %s sentence pairs\" % len(pairs))\n",
    "    for pair in pairsn:\n",
    "        if filterPairs(pair) == True:\n",
    "            pairs.append(pair)\n",
    "        else:\n",
    "            pass\n",
    "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
    "    print(\"Counting words...\")\n",
    "    for pair in pairs:\n",
    "        input_lang.addSentence(pair[0])\n",
    "        output_lang.addSentence(pair[1])\n",
    "    print(\"Counted words:\")\n",
    "    print(input_lang.name, input_lang.n_words)\n",
    "    print(output_lang.name, output_lang.n_words)\n",
    "    return input_lang, output_lang, pairs\n",
    "\n",
    "input_lang, output_lang, pairs = prepareData(pairs)\n",
    "print(random.choice(pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6ee7eba2-cc29-4331-903b-b205e33a8a41",
   "metadata": {
    "id": "6ee7eba2-cc29-4331-903b-b205e33a8a41"
   },
   "outputs": [],
   "source": [
    "def indexesFromSentence(lang, sentence):\n",
    "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
    "\n",
    "def tensorFromSentence(lang, sentence):\n",
    "    indexes = indexesFromSentence(lang, sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    return torch.tensor(indexes, dtype=torch.long, device=device).view(1, -1)\n",
    "\n",
    "def tensorsFromPair(pair):\n",
    "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
    "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
    "    return (input_tensor, target_tensor)\n",
    "\n",
    "def get_dataloader(input_lang, output_lang, pairs,batch_size):\n",
    "    n = len(pairs)\n",
    "    input_ids = np.zeros((n, MAX_LENGTH), dtype=np.int32)\n",
    "    target_ids = np.zeros((n, MAX_LENGTH), dtype=np.int32)\n",
    "\n",
    "    for idx, (inp, tgt) in enumerate(pairs):\n",
    "        inp_ids = indexesFromSentence(input_lang, inp)\n",
    "        tgt_ids = indexesFromSentence(output_lang, tgt)\n",
    "        inp_ids.append(EOS_token)\n",
    "        tgt_ids.append(EOS_token)\n",
    "        input_ids[idx, :len(inp_ids)] = inp_ids\n",
    "        target_ids[idx, :len(tgt_ids)] = tgt_ids\n",
    "\n",
    "    train_data = TensorDataset(torch.LongTensor(input_ids).to(device),\n",
    "                               torch.LongTensor(target_ids).to(device))\n",
    "\n",
    "    train_sampler = RandomSampler(train_data)\n",
    "    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "    return input_lang, output_lang, train_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5321ecaf-1628-4898-9577-1dd4dee499cf",
   "metadata": {
    "id": "5321ecaf-1628-4898-9577-1dd4dee499cf"
   },
   "outputs": [],
   "source": [
    "dl = get_dataloader(input_lang, output_lang, pairs,BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7479f2f8-f63c-4458-8959-374f6d72a9d3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7479f2f8-f63c-4458-8959-374f6d72a9d3",
    "outputId": "b4432a3f-358d-410f-d520-ed7edbe8e45f",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4848, 5608, 4959, 2671, 6014, 6022, 5725,    1,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "for batch in dl[2]:\n",
    "    print(batch[0][0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dd45a794-2d25-4878-9147-8a4add20c150",
   "metadata": {
    "id": "dd45a794-2d25-4878-9147-8a4add20c150"
   },
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, dropout_p=0.1):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "\n",
    "    def forward(self, input):\n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        output, hidden = self.gru(embedded)\n",
    "        return output, hidden\n",
    "\n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, encoder_outputs, encoder_hidden, target_tensor=None):\n",
    "        batch_size = encoder_outputs.size(0)\n",
    "        decoder_input = torch.empty(batch_size, 1, dtype=torch.long, device=device).fill_(SOS_token)\n",
    "        decoder_hidden = encoder_hidden\n",
    "        decoder_outputs = []\n",
    "\n",
    "        for i in range(MAX_LENGTH):\n",
    "            decoder_output, decoder_hidden  = self.forward_step(decoder_input, decoder_hidden)\n",
    "            decoder_outputs.append(decoder_output)\n",
    "\n",
    "            if target_tensor is not None:\n",
    "                # Teacher forcing: Feed the target as the next input\n",
    "                decoder_input = target_tensor[:, i].unsqueeze(1) # Teacher forcing\n",
    "            else:\n",
    "                # Without teacher forcing: use its own predictions as the next input\n",
    "                _, topi = decoder_output.topk(1)\n",
    "                decoder_input = topi.squeeze(-1).detach()  # detach from history as input\n",
    "\n",
    "        decoder_outputs = torch.cat(decoder_outputs, dim=1)\n",
    "        decoder_outputs = F.log_softmax(decoder_outputs, dim=-1)\n",
    "        return decoder_outputs, decoder_hidden, None # We return `None` for consistency in the training loop\n",
    "\n",
    "    def forward_step(self, input, hidden):\n",
    "        output = self.embedding(input)\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        output = self.out(output)\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dea2ed37-78ff-43cc-9cf9-2ef86631158e",
   "metadata": {
    "id": "dea2ed37-78ff-43cc-9cf9-2ef86631158e"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "45c35793-da5f-47ee-a1f0-c954d3c4391c",
   "metadata": {
    "id": "45c35793-da5f-47ee-a1f0-c954d3c4391c"
   },
   "outputs": [],
   "source": [
    "def train_epoch(dataloader, encoder, decoder, encoder_optimizer,\n",
    "          decoder_optimizer, criterion):\n",
    "\n",
    "    total_loss = 0\n",
    "    for data in dataloader:\n",
    "        input_tensor, target_tensor = data\n",
    "\n",
    "        encoder_optimizer.zero_grad()\n",
    "        decoder_optimizer.zero_grad()\n",
    "\n",
    "        encoder_outputs, encoder_hidden = encoder(input_tensor)\n",
    "        decoder_outputs, _, _ = decoder(encoder_outputs, encoder_hidden, target_tensor)\n",
    "\n",
    "        loss = criterion(\n",
    "            decoder_outputs.view(-1, decoder_outputs.size(-1)),\n",
    "            target_tensor.view(-1)\n",
    "        )\n",
    "        loss.backward()\n",
    "\n",
    "        encoder_optimizer.step()\n",
    "        decoder_optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c94033eb-cfdd-4f46-882e-dbf3db028fa1",
   "metadata": {
    "id": "c94033eb-cfdd-4f46-882e-dbf3db028fa1"
   },
   "outputs": [],
   "source": [
    "def train(train_dataloader, encoder, decoder, n_epochs, learning_rate=0.001,\n",
    "               print_every=100):\n",
    "    start = time.time()\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "    plot_loss_total = 0  # Reset every plot_every\n",
    "\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "    plot_loss_total = 0  # Reset every plot_every\n",
    "\n",
    "    encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        loss = train_epoch(train_dataloader, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "\n",
    "        if epoch % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print('%s (%d %d%%) %.4f' % (timeSince(start, epoch / n_epochs),\n",
    "                                        epoch, epoch / n_epochs * 100, print_loss_avg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c991864f-dc1c-4793-85f3-3db36039182e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c991864f-dc1c-4793-85f3-3db36039182e",
    "outputId": "d85ece4c-9366-4450-f442-5301ddc4a06a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0m 19s (- 4m 55s) (5 6%) 2.6101\n",
      "0m 35s (- 4m 9s) (10 12%) 1.9130\n",
      "0m 52s (- 3m 47s) (15 18%) 1.4971\n",
      "1m 9s (- 3m 27s) (20 25%) 1.1665\n",
      "1m 24s (- 3m 6s) (25 31%) 0.9162\n",
      "1m 43s (- 2m 51s) (30 37%) 0.7348\n",
      "1m 59s (- 2m 34s) (35 43%) 0.6010\n",
      "2m 15s (- 2m 15s) (40 50%) 0.4982\n"
     ]
    }
   ],
   "source": [
    "hidden_size = 128\n",
    "\n",
    "encoder = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
    "decoder = DecoderRNN(hidden_size, output_lang.n_words).to(device)\n",
    "\n",
    "train(dl[2], encoder, decoder, 80, print_every=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2921433b-c871-432e-b1ea-d552b488496d",
   "metadata": {
    "id": "2921433b-c871-432e-b1ea-d552b488496d"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
